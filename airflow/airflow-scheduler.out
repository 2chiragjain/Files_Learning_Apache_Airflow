[2025-09-07T22:32:48.485+0000] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2025-09-07T22:32:48.545+0000] {scheduler_job_runner.py:935} INFO - Starting the scheduler
[2025-09-07T22:32:48.549+0000] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times
[2025-09-07T22:32:48.559+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 15185
[2025-09-07T22:32:48.563+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T22:32:48.570+0000] {settings.py:63} INFO - Configured default timezone UTC
[2025-09-07T22:32:48.625+0000] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-09-07T22:32:51.638+0000] {serve_logs.py:85} WARNING - The Authorization header is missing: Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Host: localhost:8793
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36
Accept-Encoding: gzip, deflate, br, zstd
Accept-Language: en-US,en;q=0.9,th;q=0.8
Cache-Control: max-age=0
Referer: https://special-sniffle-4j96vg7xxrxx279rp.github.dev/
X-Request-Id: 34e9dd4fe9acd6f8adeb2c085e782997
X-Real-Ip: 35.142.96.234
X-Forwarded-Port: 443
X-Forwarded-Scheme: https
X-Original-Uri: /
X-Scheme: https
Dnt: 1
Sec-Fetch-Site: same-site
Sec-Fetch-Mode: navigate
Sec-Fetch-Dest: document
Sec-Ch-Ua: "Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"
Sec-Ch-Ua-Mobile: ?0
Sec-Ch-Ua-Platform: "Windows"
Priority: u=0, i
X-Original-Proto: https
X-Forwarded-Proto: https
X-Forwarded-Host: special-sniffle-4j96vg7xxrxx279rp-8793.app.github.dev
X-Forwarded-For: 35.142.96.234
Proxy-Connection: Keep-Alive

.
[2025-09-07T22:32:51.811+0000] {serve_logs.py:85} WARNING - The Authorization header is missing: Accept: image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8
Host: localhost:8793
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36
Accept-Encoding: gzip, deflate, br, zstd
Accept-Language: en-US,en;q=0.9,th;q=0.8
Referer: https://special-sniffle-4j96vg7xxrxx279rp-8793.app.github.dev/
X-Request-Id: 4383b2cb5be0f896f800c8ae8e303cbc
X-Real-Ip: 35.142.96.234
X-Forwarded-Port: 443
X-Forwarded-Scheme: https
X-Original-Uri: /favicon.ico
X-Scheme: https
Sec-Ch-Ua-Platform: "Windows"
Sec-Ch-Ua: "Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"
Dnt: 1
Sec-Ch-Ua-Mobile: ?0
Sec-Fetch-Site: same-origin
Sec-Fetch-Mode: no-cors
Sec-Fetch-Dest: image
Priority: u=1, i
X-Original-Proto: https
X-Forwarded-Proto: https
X-Forwarded-Host: special-sniffle-4j96vg7xxrxx279rp-8793.app.github.dev
X-Forwarded-For: 35.142.96.234
Proxy-Connection: Keep-Alive

.
[2025-09-07T22:37:48.631+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T22:42:48.660+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T22:47:48.687+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T22:52:48.714+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T22:57:48.754+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:02:48.796+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:03:11.754+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:03:10.879694+00:00 [scheduled]>
[2025-09-07T23:03:11.755+0000] {scheduler_job_runner.py:495} INFO - DAG hello_world has 0/16 running and queued tasks
[2025-09-07T23:03:11.755+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:03:10.879694+00:00 [scheduled]>
[2025-09-07T23:03:11.757+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:03:10.879694+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:03:11.757+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2025-09-07T23:03:10.879694+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:03:11.757+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2025-09-07T23:03:10.879694+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_hello_world.py']
[2025-09-07T23:03:11.761+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2025-09-07T23:03:10.879694+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_hello_world.py']
[2025-09-07T23:03:12.956+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/simple_hello_world.py
[2025-09-07T23:03:13.117+0000] {task_command.py:467} INFO - Running <TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:03:10.879694+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:03:13.566+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2025-09-07T23:03:10.879694+00:00', try_number=1, map_index=-1)
[2025-09-07T23:03:13.572+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=hello_world, task_id=hello_world_task, run_id=manual__2025-09-07T23:03:10.879694+00:00, map_index=-1, run_start_date=2025-09-07 23:03:13.153919+00:00, run_end_date=2025-09-07 23:03:13.279929+00:00, run_duration=0.12601, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:03:11.756021+00:00, queued_by_job_id=3, pid=28741
[2025-09-07T23:03:13.596+0000] {dagrun.py:854} INFO - Marking run <DagRun hello_world @ 2025-09-07 23:03:10.879694+00:00: manual__2025-09-07T23:03:10.879694+00:00, state:running, queued_at: 2025-09-07 23:03:10.895743+00:00. externally triggered: True> successful
[2025-09-07T23:03:13.597+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=hello_world, execution_date=2025-09-07 23:03:10.879694+00:00, run_id=manual__2025-09-07T23:03:10.879694+00:00, run_start_date=2025-09-07 23:03:11.729021+00:00, run_end_date=2025-09-07 23:03:13.597051+00:00, run_duration=1.86803, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-09-07 23:03:10.879694+00:00, data_interval_end=2025-09-07 23:03:10.879694+00:00, dag_hash=88f303ec2c9857d49be8588c02aa5640
[2025-09-07T23:05:37.010+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: hello_world.hello_world_task scheduled__2025-09-06T00:00:00+00:00 [scheduled]>
[2025-09-07T23:05:37.011+0000] {scheduler_job_runner.py:495} INFO - DAG hello_world has 0/16 running and queued tasks
[2025-09-07T23:05:37.011+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: hello_world.hello_world_task scheduled__2025-09-06T00:00:00+00:00 [scheduled]>
[2025-09-07T23:05:37.012+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: hello_world.hello_world_task scheduled__2025-09-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:05:37.013+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='scheduled__2025-09-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:05:37.013+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'scheduled__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_hello_world.py']
[2025-09-07T23:05:37.017+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'scheduled__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_hello_world.py']
[2025-09-07T23:05:38.439+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/simple_hello_world.py
[2025-09-07T23:05:38.632+0000] {task_command.py:467} INFO - Running <TaskInstance: hello_world.hello_world_task scheduled__2025-09-06T00:00:00+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:05:39.125+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='scheduled__2025-09-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-09-07T23:05:39.128+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=hello_world, task_id=hello_world_task, run_id=scheduled__2025-09-06T00:00:00+00:00, map_index=-1, run_start_date=2025-09-07 23:05:38.665570+00:00, run_end_date=2025-09-07 23:05:38.824436+00:00, run_duration=0.158866, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:05:37.011917+00:00, queued_by_job_id=3, pid=29815
[2025-09-07T23:05:39.175+0000] {dagrun.py:854} INFO - Marking run <DagRun hello_world @ 2025-09-06 00:00:00+00:00: scheduled__2025-09-06T00:00:00+00:00, state:running, queued_at: 2025-09-07 23:05:36.981182+00:00. externally triggered: False> successful
[2025-09-07T23:05:39.176+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=hello_world, execution_date=2025-09-06 00:00:00+00:00, run_id=scheduled__2025-09-06T00:00:00+00:00, run_start_date=2025-09-07 23:05:36.994539+00:00, run_end_date=2025-09-07 23:05:39.176020+00:00, run_duration=2.181481, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 00:00:00+00:00, data_interval_end=2025-09-07 00:00:00+00:00, dag_hash=ba8fb2c152734e6cddd872ecfca02267
[2025-09-07T23:07:48.825+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:12:48.863+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:17:48.891+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:22:48.918+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:27:48.945+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:32:48.966+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:37:48.993+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:39:25.959+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:39:24.577731+00:00 [scheduled]>
[2025-09-07T23:39:25.959+0000] {scheduler_job_runner.py:495} INFO - DAG hello_world has 0/16 running and queued tasks
[2025-09-07T23:39:25.960+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:39:24.577731+00:00 [scheduled]>
[2025-09-07T23:39:25.961+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:39:24.577731+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:39:25.961+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2025-09-07T23:39:24.577731+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:39:25.962+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2025-09-07T23:39:24.577731+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_hello_world.py']
[2025-09-07T23:39:25.965+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2025-09-07T23:39:24.577731+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_hello_world.py']
[2025-09-07T23:39:27.266+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/simple_hello_world.py
[2025-09-07T23:39:27.459+0000] {task_command.py:467} INFO - Running <TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:39:24.577731+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:39:27.929+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2025-09-07T23:39:24.577731+00:00', try_number=1, map_index=-1)
[2025-09-07T23:39:27.932+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=hello_world, task_id=hello_world_task, run_id=manual__2025-09-07T23:39:24.577731+00:00, map_index=-1, run_start_date=2025-09-07 23:39:27.496812+00:00, run_end_date=2025-09-07 23:39:27.620990+00:00, run_duration=0.124178, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:39:25.960678+00:00, queued_by_job_id=3, pid=43743
[2025-09-07T23:39:27.973+0000] {dagrun.py:854} INFO - Marking run <DagRun hello_world @ 2025-09-07 23:39:24.577731+00:00: manual__2025-09-07T23:39:24.577731+00:00, state:running, queued_at: 2025-09-07 23:39:24.587705+00:00. externally triggered: True> successful
[2025-09-07T23:39:27.974+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=hello_world, execution_date=2025-09-07 23:39:24.577731+00:00, run_id=manual__2025-09-07T23:39:24.577731+00:00, run_start_date=2025-09-07 23:39:25.943394+00:00, run_end_date=2025-09-07 23:39:27.974362+00:00, run_duration=2.030968, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-09-06 00:00:00+00:00, data_interval_end=2025-09-07 00:00:00+00:00, dag_hash=d74b11ae748cf8b0087ebf09915348d7
[2025-09-07T23:41:17.037+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:41:16.490482+00:00 [scheduled]>
[2025-09-07T23:41:17.038+0000] {scheduler_job_runner.py:495} INFO - DAG hello_world has 0/16 running and queued tasks
[2025-09-07T23:41:17.038+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:41:16.490482+00:00 [scheduled]>
[2025-09-07T23:41:17.040+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:41:16.490482+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:41:17.040+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2025-09-07T23:41:16.490482+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:41:17.041+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2025-09-07T23:41:16.490482+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_hello_world.py']
[2025-09-07T23:41:17.044+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2025-09-07T23:41:16.490482+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_hello_world.py']
[2025-09-07T23:41:18.235+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/simple_hello_world.py
[2025-09-07T23:41:18.390+0000] {task_command.py:467} INFO - Running <TaskInstance: hello_world.hello_world_task manual__2025-09-07T23:41:16.490482+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:41:18.850+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2025-09-07T23:41:16.490482+00:00', try_number=1, map_index=-1)
[2025-09-07T23:41:18.853+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=hello_world, task_id=hello_world_task, run_id=manual__2025-09-07T23:41:16.490482+00:00, map_index=-1, run_start_date=2025-09-07 23:41:18.424765+00:00, run_end_date=2025-09-07 23:41:18.554659+00:00, run_duration=0.129894, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:41:17.039391+00:00, queued_by_job_id=3, pid=44537
[2025-09-07T23:41:18.895+0000] {dagrun.py:854} INFO - Marking run <DagRun hello_world @ 2025-09-07 23:41:16.490482+00:00: manual__2025-09-07T23:41:16.490482+00:00, state:running, queued_at: 2025-09-07 23:41:16.496834+00:00. externally triggered: True> successful
[2025-09-07T23:41:18.895+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=hello_world, execution_date=2025-09-07 23:41:16.490482+00:00, run_id=manual__2025-09-07T23:41:16.490482+00:00, run_start_date=2025-09-07 23:41:17.021036+00:00, run_end_date=2025-09-07 23:41:18.895690+00:00, run_duration=1.874654, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-09-06 00:00:00+00:00, data_interval_end=2025-09-07 00:00:00+00:00, dag_hash=75b47e7b9802507cd5c01520ac789e62
[2025-09-07T23:42:27.376+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to None, run_after=None
[2025-09-07T23:42:27.403+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskB scheduled__2025-09-06T00:00:00+00:00 [scheduled]>
[2025-09-07T23:42:27.404+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:42:27.404+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskB scheduled__2025-09-06T00:00:00+00:00 [scheduled]>
[2025-09-07T23:42:27.405+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskB scheduled__2025-09-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:42:27.406+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='scheduled__2025-09-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:42:27.406+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'scheduled__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:42:27.410+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'scheduled__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:42:28.635+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:42:28.795+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskB scheduled__2025-09-06T00:00:00+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:42:29.273+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='scheduled__2025-09-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-09-07T23:42:29.276+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskB, run_id=scheduled__2025-09-06T00:00:00+00:00, map_index=-1, run_start_date=2025-09-07 23:42:28.834232+00:00, run_end_date=2025-09-07 23:42:28.957725+00:00, run_duration=0.123493, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:42:27.404862+00:00, queued_by_job_id=3, pid=45066
[2025-09-07T23:42:29.319+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskA scheduled__2025-09-06T00:00:00+00:00 [scheduled]>
[2025-09-07T23:42:29.319+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:42:29.319+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskA scheduled__2025-09-06T00:00:00+00:00 [scheduled]>
[2025-09-07T23:42:29.321+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskA scheduled__2025-09-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:42:29.321+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='scheduled__2025-09-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:42:29.321+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'scheduled__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:42:29.325+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'scheduled__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:42:30.500+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:42:30.658+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskA scheduled__2025-09-06T00:00:00+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:42:31.153+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='scheduled__2025-09-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-09-07T23:42:31.156+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskA, run_id=scheduled__2025-09-06T00:00:00+00:00, map_index=-1, run_start_date=2025-09-07 23:42:30.691848+00:00, run_end_date=2025-09-07 23:42:30.820335+00:00, run_duration=0.128487, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:42:29.320347+00:00, queued_by_job_id=3, pid=45078
[2025-09-07T23:42:31.236+0000] {dagrun.py:854} INFO - Marking run <DagRun executing_multiple_tasks @ 2025-09-06 00:00:00+00:00: scheduled__2025-09-06T00:00:00+00:00, state:running, queued_at: 2025-09-07 23:42:27.373224+00:00. externally triggered: False> successful
[2025-09-07T23:42:31.243+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_multiple_tasks, execution_date=2025-09-06 00:00:00+00:00, run_id=scheduled__2025-09-06T00:00:00+00:00, run_start_date=2025-09-07 23:42:27.384866+00:00, run_end_date=2025-09-07 23:42:31.243067+00:00, run_duration=3.858201, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 00:00:00+00:00, data_interval_end=2025-09-06 00:00:00+00:00, dag_hash=d6f6ff7db102fc3b152d8a28dabbe771
[2025-09-07T23:42:31.248+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to None, run_after=None
[2025-09-07T23:42:49.022+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:43:39.791+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:43:38.323116+00:00 [scheduled]>
[2025-09-07T23:43:39.791+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:43:39.791+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:43:38.323116+00:00 [scheduled]>
[2025-09-07T23:43:39.792+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:43:38.323116+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:43:39.793+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:43:38.323116+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:43:39.793+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:43:38.323116+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:43:39.797+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:43:38.323116+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:43:40.977+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:43:41.136+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:43:38.323116+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:43:41.597+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:43:38.323116+00:00', try_number=1, map_index=-1)
[2025-09-07T23:43:41.600+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskA, run_id=manual__2025-09-07T23:43:38.323116+00:00, map_index=-1, run_start_date=2025-09-07 23:43:41.172569+00:00, run_end_date=2025-09-07 23:43:41.296207+00:00, run_duration=0.123638, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:43:39.792277+00:00, queued_by_job_id=3, pid=45583
[2025-09-07T23:43:41.642+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:43:38.323116+00:00 [scheduled]>
[2025-09-07T23:43:41.643+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:43:41.643+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:43:38.323116+00:00 [scheduled]>
[2025-09-07T23:43:41.644+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:43:38.323116+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:43:41.645+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='manual__2025-09-07T23:43:38.323116+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:43:41.645+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'manual__2025-09-07T23:43:38.323116+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:43:41.649+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'manual__2025-09-07T23:43:38.323116+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:43:42.997+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:43:43.147+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:43:38.323116+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:43:43.597+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='manual__2025-09-07T23:43:38.323116+00:00', try_number=1, map_index=-1)
[2025-09-07T23:43:43.600+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskB, run_id=manual__2025-09-07T23:43:38.323116+00:00, map_index=-1, run_start_date=2025-09-07 23:43:43.185243+00:00, run_end_date=2025-09-07 23:43:43.308302+00:00, run_duration=0.123059, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:43:41.644009+00:00, queued_by_job_id=3, pid=45595
[2025-09-07T23:43:43.645+0000] {dagrun.py:854} INFO - Marking run <DagRun executing_multiple_tasks @ 2025-09-07 23:43:38.323116+00:00: manual__2025-09-07T23:43:38.323116+00:00, state:running, queued_at: 2025-09-07 23:43:38.329770+00:00. externally triggered: True> successful
[2025-09-07T23:43:43.645+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_multiple_tasks, execution_date=2025-09-07 23:43:38.323116+00:00, run_id=manual__2025-09-07T23:43:38.323116+00:00, run_start_date=2025-09-07 23:43:39.774258+00:00, run_end_date=2025-09-07 23:43:43.645513+00:00, run_duration=3.871255, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-09-07 23:43:38.323116+00:00, data_interval_end=2025-09-07 23:43:38.323116+00:00, dag_hash=ab5a91820c3d9ec06ec675b65cb15d62
[2025-09-07T23:44:11.030+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:44:21.660+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>
[2025-09-07T23:44:21.661+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:44:21.661+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>
[2025-09-07T23:44:21.663+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:44:21.663+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:44:20.672695+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 4 and queue default
[2025-09-07T23:44:21.664+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:44:20.672695+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:44:21.668+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:44:20.672695+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:44:22.924+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:44:23.191+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:44:20.672695+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:44:23.644+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:44:20.672695+00:00', try_number=1, map_index=-1)
[2025-09-07T23:44:23.647+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskA, run_id=manual__2025-09-07T23:44:20.672695+00:00, map_index=-1, run_start_date=2025-09-07 23:44:23.228201+00:00, run_end_date=2025-09-07 23:44:23.353737+00:00, run_duration=0.125536, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-09-07 23:44:21.662327+00:00, queued_by_job_id=3, pid=45900
[2025-09-07T23:44:23.693+0000] {scheduler_job_runner.py:423} INFO - 2 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>
[2025-09-07T23:44:23.694+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:44:23.694+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 1/16 running and queued tasks
[2025-09-07T23:44:23.695+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>
[2025-09-07T23:44:23.697+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>, <TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:44:23.698+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='manual__2025-09-07T23:44:20.672695+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:44:23.698+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'manual__2025-09-07T23:44:20.672695+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:44:23.699+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskC', run_id='manual__2025-09-07T23:44:20.672695+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:44:23.699+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskC', 'manual__2025-09-07T23:44:20.672695+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:44:23.703+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'manual__2025-09-07T23:44:20.672695+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:44:24.894+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:44:25.060+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:44:20.672695+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:44:29.514+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskC', 'manual__2025-09-07T23:44:20.672695+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:44:30.728+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:44:30.885+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:44:20.672695+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:44:46.351+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='manual__2025-09-07T23:44:20.672695+00:00', try_number=1, map_index=-1)
[2025-09-07T23:44:46.352+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskC', run_id='manual__2025-09-07T23:44:20.672695+00:00', try_number=1, map_index=-1)
[2025-09-07T23:44:46.355+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskB, run_id=manual__2025-09-07T23:44:20.672695+00:00, map_index=-1, run_start_date=2025-09-07 23:44:25.098218+00:00, run_end_date=2025-09-07 23:44:29.228969+00:00, run_duration=4.130751, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:44:23.696034+00:00, queued_by_job_id=3, pid=45912
[2025-09-07T23:44:46.356+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskC, run_id=manual__2025-09-07T23:44:20.672695+00:00, map_index=-1, run_start_date=2025-09-07 23:44:30.920431+00:00, run_end_date=2025-09-07 23:44:46.053640+00:00, run_duration=15.133209, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:44:23.696034+00:00, queued_by_job_id=3, pid=45965
[2025-09-07T23:44:46.502+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:44:46.520+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>
[2025-09-07T23:44:46.520+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:44:46.521+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>
[2025-09-07T23:44:46.522+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:44:20.672695+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:44:46.523+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskD', run_id='manual__2025-09-07T23:44:20.672695+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:44:46.523+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskD', 'manual__2025-09-07T23:44:20.672695+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:44:46.527+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskD', 'manual__2025-09-07T23:44:20.672695+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:44:47.724+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:44:47.903+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:44:20.672695+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:44:48.362+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskD', run_id='manual__2025-09-07T23:44:20.672695+00:00', try_number=1, map_index=-1)
[2025-09-07T23:44:48.365+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskD, run_id=manual__2025-09-07T23:44:20.672695+00:00, map_index=-1, run_start_date=2025-09-07 23:44:47.940047+00:00, run_end_date=2025-09-07 23:44:48.060914+00:00, run_duration=0.120867, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:44:46.521610+00:00, queued_by_job_id=3, pid=46080
[2025-09-07T23:44:48.509+0000] {dagrun.py:854} INFO - Marking run <DagRun executing_multiple_tasks @ 2025-09-07 23:44:20.672695+00:00: manual__2025-09-07T23:44:20.672695+00:00, state:running, queued_at: 2025-09-07 23:44:20.677387+00:00. externally triggered: True> successful
[2025-09-07T23:44:48.509+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_multiple_tasks, execution_date=2025-09-07 23:44:20.672695+00:00, run_id=manual__2025-09-07T23:44:20.672695+00:00, run_start_date=2025-09-07 23:44:21.636929+00:00, run_end_date=2025-09-07 23:44:48.509537+00:00, run_duration=26.872608, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-09-06 23:44:20.672695+00:00, data_interval_end=2025-09-07 23:44:20.672695+00:00, dag_hash=0348451d517d87a90f57a2ebef65096b
[2025-09-07T23:45:16.651+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:45:46.955+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:46:17.289+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:47:49.061+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:48:04.160+0000] {manager.py:537} INFO - DAG executing_multiple_tasks is missing and will be deactivated.
[2025-09-07T23:48:04.163+0000] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-09-07T23:48:04.168+0000] {manager.py:553} INFO - Deleted DAG executing_multiple_tasks in serialized_dag table
[2025-09-07T23:50:17.178+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:50:47.346+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:50:51.072+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:50:54.380+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:51:04.467+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:51:03.441178+00:00 [scheduled]>
[2025-09-07T23:51:04.468+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:51:04.468+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:51:03.441178+00:00 [scheduled]>
[2025-09-07T23:51:04.469+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:51:03.441178+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:51:04.470+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:51:03.441178+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-09-07T23:51:04.470+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:51:03.441178+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:51:04.474+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:51:03.441178+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:51:05.782+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:51:05.797+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskA.sh
[2025-09-07T23:51:05.797+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskB.sh
[2025-09-07T23:51:05.798+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskC.sh
[2025-09-07T23:51:05.798+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskD.sh
[2025-09-07T23:51:05.799+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskE.sh
[2025-09-07T23:51:05.799+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskF.sh
[2025-09-07T23:51:05.800+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskG.sh
[2025-09-07T23:51:05.943+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:51:03.441178+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:51:06.382+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:51:03.441178+00:00', try_number=1, map_index=-1)
[2025-09-07T23:51:06.385+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskA, run_id=manual__2025-09-07T23:51:03.441178+00:00, map_index=-1, run_start_date=2025-09-07 23:51:05.980799+00:00, run_end_date=2025-09-07 23:51:06.061389+00:00, run_duration=0.08059, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2025-09-07 23:51:04.469119+00:00, queued_by_job_id=3, pid=49245
[2025-09-07T23:51:07.412+0000] {dagrun.py:823} ERROR - Marking run <DagRun executing_multiple_tasks @ 2025-09-07 23:51:03.441178+00:00: manual__2025-09-07T23:51:03.441178+00:00, state:running, queued_at: 2025-09-07 23:51:03.445902+00:00. externally triggered: True> failed
[2025-09-07T23:51:07.412+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_multiple_tasks, execution_date=2025-09-07 23:51:03.441178+00:00, run_id=manual__2025-09-07T23:51:03.441178+00:00, run_start_date=2025-09-07 23:51:04.449138+00:00, run_end_date=2025-09-07 23:51:07.412798+00:00, run_duration=2.96366, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-09-06 23:51:03.441178+00:00, data_interval_end=2025-09-07 23:51:03.441178+00:00, dag_hash=00884def79a7f5899a57515e7fbd317c
[2025-09-07T23:51:24.780+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:51:55.006+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:52:25.421+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:52:39.732+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:52:38.910917+00:00 [scheduled]>
[2025-09-07T23:52:39.732+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:52:39.733+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:52:38.910917+00:00 [scheduled]>
[2025-09-07T23:52:39.734+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:52:38.910917+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:52:39.734+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:52:38.910917+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-09-07T23:52:39.734+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:52:38.910917+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:52:39.738+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:52:38.910917+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:52:41.190+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:52:41.204+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskA.sh
[2025-09-07T23:52:41.205+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskB.sh
[2025-09-07T23:52:41.206+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskC.sh
[2025-09-07T23:52:41.207+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskD.sh
[2025-09-07T23:52:41.208+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskE.sh
[2025-09-07T23:52:41.208+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskF.sh
[2025-09-07T23:52:41.209+0000] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/jinja2/loaders.py", line 207, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: taskG.sh
[2025-09-07T23:52:41.376+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:52:38.910917+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:52:41.804+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:52:38.910917+00:00', try_number=1, map_index=-1)
[2025-09-07T23:52:41.807+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskA, run_id=manual__2025-09-07T23:52:38.910917+00:00, map_index=-1, run_start_date=2025-09-07 23:52:41.413955+00:00, run_end_date=2025-09-07 23:52:41.489454+00:00, run_duration=0.075499, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2025-09-07 23:52:39.733519+00:00, queued_by_job_id=3, pid=49968
[2025-09-07T23:52:42.074+0000] {dagrun.py:823} ERROR - Marking run <DagRun executing_multiple_tasks @ 2025-09-07 23:52:38.910917+00:00: manual__2025-09-07T23:52:38.910917+00:00, state:running, queued_at: 2025-09-07 23:52:38.918653+00:00. externally triggered: True> failed
[2025-09-07T23:52:42.074+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_multiple_tasks, execution_date=2025-09-07 23:52:38.910917+00:00, run_id=manual__2025-09-07T23:52:38.910917+00:00, run_start_date=2025-09-07 23:52:39.713126+00:00, run_end_date=2025-09-07 23:52:42.074549+00:00, run_duration=2.361423, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-09-06 23:52:38.910917+00:00, data_interval_end=2025-09-07 23:52:38.910917+00:00, dag_hash=3de2273005cc2a8d0879a8d3413bf89e
[2025-09-07T23:52:49.088+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-09-07T23:52:56.684+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:53:27.597+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:53:31.218+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:53:34.769+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
[2025-09-07T23:53:34.770+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:53:34.770+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
[2025-09-07T23:53:34.771+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:53:34.772+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-09-07T23:53:34.772+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:34.776+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:36.064+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:53:36.221+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:53:33.927649+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:53:36.664+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1)
[2025-09-07T23:53:36.667+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskA, run_id=manual__2025-09-07T23:53:33.927649+00:00, map_index=-1, run_start_date=2025-09-07 23:53:36.256127+00:00, run_end_date=2025-09-07 23:53:36.382916+00:00, run_duration=0.126789, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2025-09-07 23:53:34.771156+00:00, queued_by_job_id=3, pid=50427
[2025-09-07T23:53:36.711+0000] {scheduler_job_runner.py:423} INFO - 3 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
[2025-09-07T23:53:36.711+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:53:36.712+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 1/16 running and queued tasks
[2025-09-07T23:53:36.712+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 2/16 running and queued tasks
[2025-09-07T23:53:36.712+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
[2025-09-07T23:53:36.714+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>, <TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>, <TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:53:36.714+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:53:36.714+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:36.715+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskC', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:53:36.715+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskC', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:36.715+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskD', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:53:36.716+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskD', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:36.720+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:38.011+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:53:38.178+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:53:33.927649+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:53:42.655+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskC', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:43.862+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:53:44.022+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:53:33.927649+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:53:54.511+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskD', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:55.802+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:53:55.980+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:53:33.927649+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:53:56.456+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1)
[2025-09-07T23:53:56.456+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskC', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1)
[2025-09-07T23:53:56.457+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskD', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1)
[2025-09-07T23:53:56.460+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskB, run_id=manual__2025-09-07T23:53:33.927649+00:00, map_index=-1, run_start_date=2025-09-07 23:53:38.223910+00:00, run_end_date=2025-09-07 23:53:42.366389+00:00, run_duration=4.142479, state=skipped, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:53:36.713384+00:00, queued_by_job_id=3, pid=50458
[2025-09-07T23:53:56.460+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskC, run_id=manual__2025-09-07T23:53:33.927649+00:00, map_index=-1, run_start_date=2025-09-07 23:53:44.076129+00:00, run_end_date=2025-09-07 23:53:54.197738+00:00, run_duration=10.121609, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=20, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:53:36.713384+00:00, queued_by_job_id=3, pid=50493
[2025-09-07T23:53:56.461+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskD, run_id=manual__2025-09-07T23:53:33.927649+00:00, map_index=-1, run_start_date=2025-09-07 23:53:56.015626+00:00, run_end_date=2025-09-07 23:53:56.146293+00:00, run_duration=0.130667, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:53:36.713384+00:00, queued_by_job_id=3, pid=50584
[2025-09-07T23:53:56.625+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskG manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
[2025-09-07T23:53:56.625+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:53:56.625+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskG manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>
[2025-09-07T23:53:56.627+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskG manual__2025-09-07T23:53:33.927649+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:53:56.627+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskG', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:53:56.627+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskG', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:56.631+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskG', 'manual__2025-09-07T23:53:33.927649+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:53:57.787+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:53:57.945+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskG manual__2025-09-07T23:53:33.927649+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:53:58.573+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskG', run_id='manual__2025-09-07T23:53:33.927649+00:00', try_number=1, map_index=-1)
[2025-09-07T23:53:58.576+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskG, run_id=manual__2025-09-07T23:53:33.927649+00:00, map_index=-1, run_start_date=2025-09-07 23:53:57.998250+00:00, run_end_date=2025-09-07 23:53:58.195240+00:00, run_duration=0.19699, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:53:56.626370+00:00, queued_by_job_id=3, pid=50602
[2025-09-07T23:53:58.615+0000] {dagrun.py:823} ERROR - Marking run <DagRun executing_multiple_tasks @ 2025-09-07 23:53:33.927649+00:00: manual__2025-09-07T23:53:33.927649+00:00, state:running, queued_at: 2025-09-07 23:53:33.935183+00:00. externally triggered: True> failed
[2025-09-07T23:53:58.616+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_multiple_tasks, execution_date=2025-09-07 23:53:33.927649+00:00, run_id=manual__2025-09-07T23:53:33.927649+00:00, run_start_date=2025-09-07 23:53:34.751335+00:00, run_end_date=2025-09-07 23:53:58.616185+00:00, run_duration=23.86485, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-09-06 23:53:33.927649+00:00, data_interval_end=2025-09-07 23:53:33.927649+00:00, dag_hash=3de2273005cc2a8d0879a8d3413bf89e
[2025-09-07T23:54:01.592+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:54:32.200+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:54:35.334+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
[2025-09-07T23:54:35.334+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:54:35.334+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
[2025-09-07T23:54:35.336+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:54:35.336+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2025-09-07T23:54:35.336+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:35.340+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskA', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:36.541+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:54:36.699+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskA manual__2025-09-07T23:54:34.886386+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:54:37.265+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskA', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1)
[2025-09-07T23:54:37.269+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskA, run_id=manual__2025-09-07T23:54:34.886386+00:00, map_index=-1, run_start_date=2025-09-07 23:54:36.733923+00:00, run_end_date=2025-09-07 23:54:36.857190+00:00, run_duration=0.123267, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=23, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2025-09-07 23:54:35.335357+00:00, queued_by_job_id=3, pid=50884
[2025-09-07T23:54:37.310+0000] {scheduler_job_runner.py:423} INFO - 3 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
[2025-09-07T23:54:37.310+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:54:37.311+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 1/16 running and queued tasks
[2025-09-07T23:54:37.311+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 2/16 running and queued tasks
[2025-09-07T23:54:37.311+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
[2025-09-07T23:54:37.313+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>, <TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>, <TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:54:37.313+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:54:37.313+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:37.314+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskC', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:54:37.314+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskC', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:37.314+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskD', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-09-07T23:54:37.315+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskD', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:37.318+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskB', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:38.499+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:54:38.658+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskB manual__2025-09-07T23:54:34.886386+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:54:43.181+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskC', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:44.384+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:54:44.535+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskC manual__2025-09-07T23:54:34.886386+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:54:55.044+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskD', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:56.373+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:54:56.528+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskD manual__2025-09-07T23:54:34.886386+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:54:56.980+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskB', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1)
[2025-09-07T23:54:56.981+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskC', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1)
[2025-09-07T23:54:56.981+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskD', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1)
[2025-09-07T23:54:56.984+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskB, run_id=manual__2025-09-07T23:54:34.886386+00:00, map_index=-1, run_start_date=2025-09-07 23:54:38.699781+00:00, run_end_date=2025-09-07 23:54:42.899317+00:00, run_duration=4.199536, state=skipped, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=24, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:54:37.312316+00:00, queued_by_job_id=3, pid=50896
[2025-09-07T23:54:56.985+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskC, run_id=manual__2025-09-07T23:54:34.886386+00:00, map_index=-1, run_start_date=2025-09-07 23:54:44.570602+00:00, run_end_date=2025-09-07 23:54:54.757838+00:00, run_duration=10.187236, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:54:37.312316+00:00, queued_by_job_id=3, pid=50949
[2025-09-07T23:54:56.985+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskD, run_id=manual__2025-09-07T23:54:34.886386+00:00, map_index=-1, run_start_date=2025-09-07 23:54:56.563492+00:00, run_end_date=2025-09-07 23:54:56.686069+00:00, run_duration=0.122577, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=26, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-09-07 23:54:37.312316+00:00, queued_by_job_id=3, pid=51042
[2025-09-07T23:54:57.251+0000] {scheduler_job_runner.py:423} INFO - 2 tasks up for execution:
	<TaskInstance: executing_multiple_tasks.taskF manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskG manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
[2025-09-07T23:54:57.251+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 0/16 running and queued tasks
[2025-09-07T23:54:57.251+0000] {scheduler_job_runner.py:495} INFO - DAG executing_multiple_tasks has 1/16 running and queued tasks
[2025-09-07T23:54:57.252+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: executing_multiple_tasks.taskF manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
	<TaskInstance: executing_multiple_tasks.taskG manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>
[2025-09-07T23:54:57.253+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: executing_multiple_tasks.taskF manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>, <TaskInstance: executing_multiple_tasks.taskG manual__2025-09-07T23:54:34.886386+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:54:57.253+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskF', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:54:57.254+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskF', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:57.254+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskG', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:54:57.254+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskG', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:57.259+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskF', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:54:58.449+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:54:58.611+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskF manual__2025-09-07T23:54:34.886386+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:54:59.056+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'executing_multiple_tasks', 'taskG', 'manual__2025-09-07T23:54:34.886386+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_multiple_tasks.py']
[2025-09-07T23:55:00.455+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_multiple_tasks.py
[2025-09-07T23:55:00.612+0000] {task_command.py:467} INFO - Running <TaskInstance: executing_multiple_tasks.taskG manual__2025-09-07T23:54:34.886386+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:55:01.061+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskF', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1)
[2025-09-07T23:55:01.062+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='executing_multiple_tasks', task_id='taskG', run_id='manual__2025-09-07T23:54:34.886386+00:00', try_number=1, map_index=-1)
[2025-09-07T23:55:01.065+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskF, run_id=manual__2025-09-07T23:54:34.886386+00:00, map_index=-1, run_start_date=2025-09-07 23:54:58.648027+00:00, run_end_date=2025-09-07 23:54:58.776110+00:00, run_duration=0.128083, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:54:57.252729+00:00, queued_by_job_id=3, pid=51055
[2025-09-07T23:55:01.065+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=executing_multiple_tasks, task_id=taskG, run_id=manual__2025-09-07T23:54:34.886386+00:00, map_index=-1, run_start_date=2025-09-07 23:55:00.649395+00:00, run_end_date=2025-09-07 23:55:00.778920+00:00, run_duration=0.129525, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-09-07 23:54:57.252729+00:00, queued_by_job_id=3, pid=51086
[2025-09-07T23:55:01.102+0000] {dagrun.py:854} INFO - Marking run <DagRun executing_multiple_tasks @ 2025-09-07 23:54:34.886386+00:00: manual__2025-09-07T23:54:34.886386+00:00, state:running, queued_at: 2025-09-07 23:54:34.894578+00:00. externally triggered: True> successful
[2025-09-07T23:55:01.103+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=executing_multiple_tasks, execution_date=2025-09-07 23:54:34.886386+00:00, run_id=manual__2025-09-07T23:54:34.886386+00:00, run_start_date=2025-09-07 23:54:35.315777+00:00, run_end_date=2025-09-07 23:55:01.103296+00:00, run_duration=25.787519, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-09-06 23:54:34.886386+00:00, data_interval_end=2025-09-07 23:54:34.886386+00:00, dag_hash=8d8253b1cf6ffacd91a1727a3800e645
[2025-09-07T23:55:03.251+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:55:34.481+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:56:04.962+0000] {dag.py:4180} INFO - Setting next_dagrun for executing_multiple_tasks to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:56:10.828+0000] {dag.py:4180} INFO - Setting next_dagrun for execute_python_operators to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
[2025-09-07T23:56:10.852+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: execute_python_operators.python_task scheduled__2025-09-06T00:00:00+00:00 [scheduled]>
[2025-09-07T23:56:10.853+0000] {scheduler_job_runner.py:495} INFO - DAG execute_python_operators has 0/16 running and queued tasks
[2025-09-07T23:56:10.853+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: execute_python_operators.python_task scheduled__2025-09-06T00:00:00+00:00 [scheduled]>
[2025-09-07T23:56:10.854+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: execute_python_operators.python_task scheduled__2025-09-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-09-07T23:56:10.855+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='execute_python_operators', task_id='python_task', run_id='scheduled__2025-09-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-09-07T23:56:10.855+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'execute_python_operators', 'python_task', 'scheduled__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2025-09-07T23:56:10.859+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'execute_python_operators', 'python_task', 'scheduled__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/execute_python_operators.py']
[2025-09-07T23:56:12.046+0000] {dagbag.py:588} INFO - Filling up the DagBag from /workspaces/Files_Learning_Apache_Airflow/airflow/dags/execute_python_operators.py
[2025-09-07T23:56:12.254+0000] {task_command.py:467} INFO - Running <TaskInstance: execute_python_operators.python_task scheduled__2025-09-06T00:00:00+00:00 [queued]> on host codespaces-9bd7b3
[2025-09-07T23:56:12.716+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='execute_python_operators', task_id='python_task', run_id='scheduled__2025-09-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-09-07T23:56:12.719+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=execute_python_operators, task_id=python_task, run_id=scheduled__2025-09-06T00:00:00+00:00, map_index=-1, run_start_date=2025-09-07 23:56:12.288672+00:00, run_end_date=2025-09-07 23:56:12.416376+00:00, run_duration=0.127704, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-09-07 23:56:10.853876+00:00, queued_by_job_id=3, pid=51633
[2025-09-07T23:56:12.744+0000] {dagrun.py:854} INFO - Marking run <DagRun execute_python_operators @ 2025-09-06 00:00:00+00:00: scheduled__2025-09-06T00:00:00+00:00, state:running, queued_at: 2025-09-07 23:56:10.824657+00:00. externally triggered: False> successful
[2025-09-07T23:56:12.744+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=execute_python_operators, execution_date=2025-09-06 00:00:00+00:00, run_id=scheduled__2025-09-06T00:00:00+00:00, run_start_date=2025-09-07 23:56:10.836390+00:00, run_end_date=2025-09-07 23:56:12.744614+00:00, run_duration=1.908224, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 00:00:00+00:00, data_interval_end=2025-09-07 00:00:00+00:00, dag_hash=9ce89bec64913621540841e815f9f2ba
[2025-09-07T23:56:12.747+0000] {dag.py:4180} INFO - Setting next_dagrun for execute_python_operators to 2025-09-07 00:00:00+00:00, run_after=2025-09-08 00:00:00+00:00
